---
title: "Survival Analysis with mlr3's Blackboost"
author: "Andrea Cutrera, Sonia Petrini, Ruben Popper, Elisabetta Rocchetti"
date: "5/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(survivalmodels)
library(mlr3proba); library(mlr3); library(survival)
library(mlr3misc)
library(survival)
library(mboost)
library(mlr3learners)
library(mlr3extralearners)
```

## Introduction

The aim of this project is to provide an example of the usage of the mlr3's tool. In particular, the method we will use is "surv.blackboost" ^[[mlr_learners_surv.blackboost](https://rdrr.io/github/mlr-org/mlr3extralearners/man/mlr_learners_surv.blackboost.html)], which combines a CART algorithm (Classification And Regression Tree) with Gradient Boosting, on a survival analysis task; for this purpose we will use the German Breast Cancer Study (GBCS) Dataset from Hosmer et al.

This method calls the blackboost function included in the mboost package.

The structure of the data is the following:

* observations: 686 - patients with primary node positive breast cancer.
* variables: 16 - biological factors concerning patients and tumor severity.
  
The Survival Analysis' dimensions are stored in these variables:

* censdead: Censoring status. 1 = Death. 0 = Censored.
* survtime: Time to death (days).

___

## surv_blackboost_mlr3 

The function `surv.blackboost` implements a gradient boosting algorithm that can be fitted for censored data. Similarly to other packages, blackboost implements the classical algorithm with regression trees as base learners.\
The peculiarity of blackboost is the possibility to change the default loss function to be optimized for the terminal regions of each tree. Blackboost allows to do so either by specifying a custom loss function or by selecting a pre-existing one under the `family` argument.\
Moreover, the performances of the model can be implemented further by specifying a variety of tree controls. In fact, blackboost relies on `partykit::ctree`, which offers a wide range of parameters for conditional inference trees.\
It is possible to type `args(partykit::ctree_control)` for more details on the tree controls parameters.

`mlr3` offers an intuitive and interconnected set of extension packages that we are going to use with respect to different aspects of this explanatory project.

We can start off by installing all the required packages:

``` {r, eval=FALSE}
install.packages(c("mlr3", "mlr3learners", "mlr3extralearners", "mlr3tuning", "mlr3proba", "mlr3misc", "survivalmodels", "mboost", "paradox", "mlr3viz", "parallel", "stabs", "mlr3pipelines", "mlr3misc", "mlr3benchmark"), repos = "http://cran.us.r-project.org")
```

___

## Packages Overview

Let us briefly discuss the role of some of the packages in *mlr3*.\
`mlr3learners` and `mlr3extralearners` include a collection of machine learning algorithms to perform classification, regression, clustering, density clustering and survival analysis task^[[mlr3learners](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html)].\
In order to declare the type of data we are dealing with we use `mlr3proba`, which allows us to create `task` objects. Tasks encapsulate all the information needed to identify the data used and the type of tasks to be performed on it.\
To perform Survival Analysis and deal with censored observations, we have to create a new `TaskSurv` instance (code examples are provided below).\
Moving on to the configuration of our algorithm, we use `paradox` and `mlr3tuning` to tune the parameters of the tree and of the boosting procedure. The former is an intuitive reference-based objects tool for the specification of the parameter set, while `mlr3tuning` takes this object and defines a space where we can optimize our parameters in line with our "searching" criteria.\
The mlr3 framework also provides a useful visualization tool through the `mlr3viz` library, which is built on ggplot2.

___

## Data Preprocessing

Before proceeding with the analysis, we load the data and provide a brief description of it.

```{r}
gbcs <- mlr3proba::gbcs
str(gbcs)
```

```{r, include = FALSE}
description <- c("Identification Code.","Date of diagnosis.","Date of recurrence free survival.","Date of death.","Age at diagnosis (years).",
           "Menopausal status. 1 = Yes, 0 = No.", "Hormone therapy. 1 = Yes. 0 = No.","Tumor size (mm).","Tumor grade (1-3).","Number of nodes.",
           "Number of progesterone receptors.","Number of estrogen receptors.","Time to recurrence (days).","Recurrence status. 1 = Recurrence. 0 = Censored.", "Time to death (days).", "Censoring status. 1 = Death. 0 = Censored.")
data.frame("variable" = colnames(gbcs),description)
```

Once we have selected the variables of interest (age, menopause, hormone, size, grades, nodes, prog_recp, estrg_recp survtime and censdead), what we do is the scaling of the variables.

In addition, we re-encode the factor "grade" with three dummy variables.

```{r}
gbcs <- gbcs[,c(5:12,15:16)]

gbcs$menopause <- gbcs$menopause-1
gbcs$hormone <- gbcs$hormone-1

gbcs$grade1 <- ifelse(gbcs$grade==1, 1,0)
gbcs$grade2 <- ifelse(gbcs$grade==2, 1,0)
gbcs$grade3 <- ifelse(gbcs$grade==3, 1,0)
gbcs$grade <- NULL

gbcs$size <- scale(gbcs$size)
gbcs$age <- scale(gbcs$age)
gbcs$nodes <- scale(gbcs$nodes)
gbcs$prog_recp <- scale(gbcs$prog_recp)
gbcs$estrg_recp <- scale(gbcs$estrg_recp)
```

In order to eventually validate our learner we need to split the dataset into train set and test set.

```{r}
set.seed(123)
train_set = sample(nrow(gbcs), 0.8 * nrow(gbcs))
test_set = setdiff(seq_len(nrow(gbcs)), train_set)

train_gbcs <- gbcs[train_set, ]

test_gbcs <- gbcs[test_set, ]
```

___

First, we need to define the tasks which will be used during the training, testing, and hyperparameters tuning procedures.

```{r}
train_task = TaskSurv$new(id = "train_gbcs", 
                          backend = train_gbcs, 
                          time = "survtime", 
                          event = "censdead")

test_task = TaskSurv$new(id = "test_gbcs", 
                         backend = test_gbcs, 
                         time = "survtime", 
                         event = "censdead")

gbcs_task = TaskSurv$new(id = "gbcs", 
                         backend = gbcs, 
                         time = "survtime", event = "censdead")
```

`mlr3viz` allows to plot the survival function by calling the task.

```{r}
library("mlr3viz")
autoplot(gbcs_task)
```

___

## Designing the learner

In this section, we are going to set the building blocks of the blackboost learner, and to dive deeper in its functionality by taking a look at its parameters and their respective default values.

```{r, echo=FALSE} 
library(mlr3extralearners)
library(mlr3proba)
library(mlr3tuning)
library(paradox)

install_learners('surv.blackboost')
learner.bb = lrn("surv.blackboost", id="blackboost", predict_sets=c("train","test"))
```

Notice that, within the `lrn()` function, the optional arguments `id` and `predict_sets` are functional to the benchmarking operation.\
`id` allows to identify the learner, while `predict_sets` provides an internal splitting to evaluate the performance on both the training and the test sets.

___

## Hyperparameters setup

Like all the other machine learning algorithms, survival blackboost has its own hyperparameters to tune.\
They comprehend both classical boosting parameters and classical learning tree parameters.

In the following we describe all the parameters which could be tuned in order to get an optimal performance from survival blackboost.\
We show a subset of all the possible information given by the command `learner$param_set`: here we list id, class (which is important to know in order to correctly specify the parameter class in search spaces, see section "Hyperparameter tuning" below), lower (bound), upper (bound), levels (all possible character values for categorical parameters), default (value) and storage_type (type of parameter).

Notice that the next output is obtained by further processing of `learner.bb$param_set` output. For more details on how to get this exact output, see the raw file.
```{r echo=FALSE}
#here we substitute the list objects stored in levels and default with their string representation to get a better visualization

param_set<-data.frame(levels=list(rep(0,(length(learner.bb$param_set)-1))),default=list(rep(0,(length(learner.bb$param_set)-1))))
for(i in 1:(length(learner.bb$param_set)-1)){
  param_set$levels[i] <- paste(learner.bb$param_set$levels[[i]], collapse = ', ')
  if(length(learner.bb$param_set$default[names(learner.bb$param_set$levels)[i]][[1]])!=0 & typeof(learner.bb$param_set$default[names(learner.bb$param_set$levels)[i]][[1]])!="closure")
  {
    param_set$default[i] <- learner.bb$param_set$default[names(learner.bb$param_set$levels)[i]][[1]]
  }
  else{
    param_set$default[i] <- "NA"
  }
}

params <- data.frame(class = unname(learner.bb$param_set$class), lower = unname(learner.bb$param_set$lower), unname(learner.bb$param_set$upper), levels = param_set$levels, default = param_set$default)
print(params)
```

Possible parameters are:

* `family` is a symbolic description of the loss function and the corresponding risk functions to be optimized by blackboost [default: coxph].
* `mstop` is an integer giving the number of initial boosting iterations. If mstop = 0, the offset model is returned (in our case, a tree is returned) [default: 100].
* `nu` is a double (between 0 and 1) defining the step size or shrinkage parameter [default: 0.1].
* `stopintern` is a logical that defines if the boosting algorithm stops internally when the outof-bag risk in one iteration is larger than the out-of-bag risk in the iteration before. Can also be a positive number giving the risk difference that needs to be exceeded [default: FALSE].
* `maxdepth` is the maximum depth of the tree [default: Inf]. 
* `trace` is a logical triggering printout of status information during the fitting process; it is just for printing out information [default: FALSE].

In order to set the learner parameters it is necessary to use the following specification, changing "..." with the desired value.

```{r eval=FALSE, message=FALSE, warning=FALSE}
learner.bb$param_set$values <- list(family = ..., 
                                    mstop = ..., 
                                    nu = ..., 
                                    stopintern = ...,
                                    trace = ..., 
                                    maxdepth = ...)
```

If one is interested in setting other parameters which are not covered in this tutorial, we suggest to check these CRAN documentations:

* blackboost parameters: https://cran.r-project.org/web/packages/mboost/mboost.pdf under "blackboost" section;
* boosting parameters: https://cran.r-project.org/web/packages/mboost/mboost.pdf under "boost_control" section;
* tree parameters: https://cran.r-project.org/web/packages/partykit/partykit.pdf under "ctree_controls" section.

Please notice that these documentations do not refer directly to blackboost learner parameters, but to the mboost package on which the learner is built.

___

## Parameters' Tuning

We compare different specifications of the hyperparameters which we believe can improve the predictive performance of our model.

First, we declare the learner which will be optimized by the following procedure.

```{r}
learner.bb.opt <- lrn("surv.blackboost", id="blackboost.opt", predict_sets=c("train","test"))
```

We define a *tuning instance* that we will later feed to the optimization algorithm of our choice.\
To do so, we specify the dataset (or the *task*, according to `mlr3proba` notation) on which tuning has to be performed, the *learner* that we are using, and the *search space*, which includes lower and upper bounds for our hyperparameters.\
Next, we need to indicate the *measure* and the *resampling* strategy according to which the performance of our model will be evaluated.\
Finally, we impose a *Terminator* - or a budget - representing the criterium according to which the optimization algorithm will stop.

```{r, results='hide'}
search_space = ps(
  mstop = p_int(lower = 70, upper = 130),
  nu = p_dbl(lower = 0.01, upper = 0.1))

CVstrat = rsmp("cv", folds = 10)

measure = msr("surv.cindex")
```

Regarding the `terminator` of our algorithm, `mlr3tuning` provides different options^[https://mlr3book.mlr-org.com/tuning.html]:

* Terminate after a given time (TerminatorClockTime)
* Terminate after a given amount of iterations (TerminatorEvals)
* Terminate after a specific performance is reached (TerminatorPerfReached)
* Terminate when tuning does not improve (TerminatorStagnation)
* A combination of the above in an ALL or ANY fashion (TerminatorCombo)

Here we present an example in which we let termination an open choice among Stagnation and Evals:

```{r, results='hide'}
combo <- trm("combo",
              list(trm("evals", n_evals = 50),
                   trm("stagnation")),
              any = TRUE)
```

We can now create the tuning instance and see what it looks like:

```{r}
instance = TuningInstanceSingleCrit$new(
  task = train_task,
  measure = measure,
  learner = learner.bb.opt,
  resampling = CVstrat,
  search_space = search_space,
  terminator = combo
)
instance
```

Finally, we can specify the optimization algorithm to be implemented on the `instance` via the `tuner` class. Of course, it is possible to choose among the set of available tuners.

```{r, results='hide'}
tuner = tnr("random_search")
tuner$optimize(instance)
```

We can now access the resulting optimized parameters which are stored in `instance`, and assign them to our new (optimized) learner:

```{r, results='hide'}
instance$result_learner_param_vals

#assigning optimized parameters
learner.bb.opt$param_set$values = instance$result_learner_param_vals
```
___

## Benchmarking

Once the new learner is configured with the optimized parameters, we are of course interested in comparing its performance with both the base blackboosting and the benchmark Cox regression.\
The mlr3 framework provides a convenient way of doing so through the `mlr3benchmark` package, which compares the performance of multiple learners, on a common task and with the same training and test data.

This package exploits two main functions:

* `benchmark_grid()` allows to set up a customized benchmarking design, composed of a combination of `Task`, `Learner`, and `Resampling` objects.
* `benchmark()` performs the benchmarking according to the specified design and returns a <BenchmarkResult>.

First, we create our benchmark, which will be a classical Cox model.

```{r}
learner.cox = lrn("surv.coxph", id="cox", predict_sets=c("train","test"))
```

Second, we define the objects that will make up the design.

```{r}
learners = c(learner.cox,learner.bb,learner.bb.opt)
print(learners)

cv3 = rsmps("cv", folds = 3)
```

Then, we build the benchmark design and pass it to `benchmark()` to implement the benchmarking.

```{r}
set.seed(123)
design <- benchmark_grid(tasks = gbcs_task,
                         learners = learners,
                         resamplings = cv3)
benchmark <- benchmark(design)
```

We can now specify the performance measures according to which the comparison should be carried out.\
Here we define two measures, Harrell's C-index on training set and on test set. 

```{r}
measures = list(
  msr("surv.cindex", predict_sets = "train", id = "cindex_train"),
  msr("surv.cindex", id = "cindex_test")
)
```

Finally, we can aggregate the cross-validated C-indices, and print a table displaying the test and train results for each learner.

```{r}
performance_tab <- benchmark$aggregate(measures)
print(performance_tab)
```

Thanks to the `mlr3viz` package it is also possible to graphically display the benchmarking results.

```{r}
autoplot(benchmark)
```

___

## Conclusions

In this project we provided an insight of the mlr3 learner `surv.blackboost`, and of its implementation in the survival analysis framework.\
Facing this case study, we have shown how to perform classical machine learning procedures with the tools featured in mlr3, such as hyperparameters' optimization.\
The performance obtained with the default `surv.blackboost` learner (without setting any parameter) is far above the one obtained with the base Cox regression. However, effectively tuning the hyperparameters requires a more sophisticated study, which is beyond the scope of our project.





