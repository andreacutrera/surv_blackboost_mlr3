---
title: "Survival Analysis with mlr3's Blackboost"
author: "Andrea Cutrera, Sonia Petrini, Elisabetta Rocchetti, Ruben Popper"
date: "5/28/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(survivalmodels)
library(mlr3proba); library(mlr3); library(survival)
library(mlr3misc)
library(survival)
library(mboost)
library(mlr3learners)
library(mlr3extralearners)
```

## Introduction

The aim of this project is to perform Survival Analysis on the German Breast Cancer Study (GBCS) Dataset from Hosmer et al.  
The machine learning tool that will be implemented is the mlr3 extra learner "surv.blackboost" (documentation available at: [mlr_learners_surv.blackboost](https://rdrr.io/github/mlr-org/mlr3extralearners/man/mlr_learners_surv.blackboost.html)), 
which combines a CART algorithm with Gradient Boosting.

This method is called blackboost and is included in the mboost package.


The structure of the data is the following:
  - observations: 686 - patients with primary node positive breast cancer.
  - variables: 16 - biological factors concerning patients and tumor severity.
  
The Survival Analysis' dimensions are stored in these variables:
  - censdead: Censoring status. 1 = Death. 0 = Censored.
  - survtime: Time to death (days).

## Data Preprocessing


```{r}
description <- c("Identification Code.","Date of diagnosis.","Date of recurrence free survival.","Date of death.","Age at diagnosis (years).",
           "Menopausal status. 1 = Yes, 0 = No.", "Hormone therapy. 1 = Yes. 0 = No.","Tumor size (mm).","Tumor grade (1-3).","Number of nodes.",
           "Number of progesterone receptors.","Number of estrogen receptors.","Time to recurrence (days).","Recurrence status. 1 = Recurrence. 0 = Censored.", "Time to death (days).", "Censoring status. 1 = Death. 0 = Censored.")
data.frame("variable" = colnames(gbcs),description)
```

Once selected the variables of interest (age, menopause, hormone, size, grades, nodes, prog_recp, estrg_recp survtime and censdead) what we do in our preprocessing phase is the scaling in order to get the same scale in all the variables.


```{r cars}
#load dataset
gbcs <- mlr3proba::gbcs
#select relevant variables 
gbcs <- gbcs[,c(5:12,15:16)]
summarytools::dfSummary(gbcs, 
                        graph.col = F, 
                        valid.col = F
)
```


Once we have our dataset ready, we make the usual random division into train and test set before creating the task for both the sets.


```{r}
set.seed(123)
train_set = sample(nrow(gbcs), 0.8 * nrow(gbcs))
test_set = setdiff(seq_len(nrow(gbcs)), train_set)

train_gbcs <- gbcs[train_set, ]
test_gbcs <- gbcs[test_set, ]
```

```{r}
train_task = TaskSurv$new(id = "train_gbcs", 
                          backend = train_gbcs, 
                          time = "survtime", 
                          event = "censdead")

test_task = TaskSurv$new(id = "test_gbcs", 
                         backend = test_gbcs, 
                         time = "survtime", 
                         event = "censdead")
```


## surv_blackboost_mlr3 

The function `surv.blackboost` implements a gradient boosting algorithm that can be fitted for censored data. Similarly to other packages, blackboost implements the classical algorithm with regression tree as base learner. The peculiarity of blackboost is the possibility to change the default loss function to be optimized for the terminal regions of each tree. Blackboost allows to do so either by specifying a custom loss function or by selecting a pre-existing one under the `family` argument. Moreover, the performances of the model can be implemented further by specifying a variety of tree controls. In fact, blackboost relies on `partykit::ctree`, which offers a wide range of parameters for conditional inference trees. You can type `args(partykit::ctree_control)` for more details on the tree controls parameters.

`mlr3` offers an intuitive and interconnected set of extension packages that we are going to use with respect to different aspects of this explanatory project.
We can start off by installing all the required packages:

``` {r, eval=FALSE}
install.packages(c("mlr3", "mlr3learners", "mlr3extralearners", "mlr3tuning", "mlr3proba", "mlr3pipelines", "mlr3misc", "survivalmodels", "mboost", "paradox", "parallell", "stabs"), repos = "http://cran.us.r-project.org")
```
___

## Packages Overview

Let us briefly discuss the role of some of the packages in *mlr3*. `mlr3learners` and `mlr3extralearners` include a collection of machine learning algorithms to perform classification, regression, clustering, density clustering and survival analysis task (available at [mlr3learners](https://mlr3extralearners.mlr-org.com/articles/learners/list_learners.html)). In order to tell R what type of data we are dealing with we use `mlr3proba`. This package facilitates our job in many ways, since it allows us to collect datasets in tasks of which it recognizes the main features. After importing and preprocessing our datasets, we can set `TaskSurv` in order to let R consider censored observation (more code examples are provided below). Moving on to the configuration of our algorithm, we use `paradox` and `mlr3tuning` for tuning the parameters of the tree and of the boosting procedure. The former is an intuitive reference-based objects tool for the specification of the parameter set. `mlr3tuning` takes this object and defines a space where we can optimize our parameters in line with our "searching" criteria.

Before moving on with the next topic, let us dive deeper in the functionality of blackboost by taking a look at its parameters and their respective default values:

```{r, echo=FALSE} 
library(mlr3extralearners)
library(mlr3proba)
library(mlr3tuning)
library(paradox)
install_learners('surv.blackboost')
learner = lrn("surv.blackboost")
learner$param_set$ids()
```
___


# Hyperparameter setup
Like all the other machine learning algorithms, survival blackboost has its own hyperparameters to tune.
They comprehend both classical boosting parameters and classical learning tree parameters.

In the following we describe all the parameters needed to be tuned in order to get the optimal performance from survival blackboost.
We show a subset of all the possible information given by the command `learner$param_set`: here we list id, class (which is important to know in order to correctly specify the parameter class in search spaces, see section "Hyperparameter tuning" below), lower (bound), upper (bound), levels (all possible character value for categorical parameters), default (value) and storage_type (type of parameter).

```{r eval=FALSE, include=FALSE}
param_set<-data.frame(levels=list(rep(0,(length(learner$param_set)-1))),default=list(rep(0,(length(learner$param_set)-1))))
for(i in 1:(length(learner$param_set)-1)){
  param_set$levels[i] <- paste(learner$param_set$levels[[i]], collapse = ', ')
  if(length(learner$param_set$default[names(learner$param_set$levels)[i]][[1]])!=0)
  {
    param_set$default[i] <- learner$param_set$default[names(learner$param_set$levels)[i]][[1]]
  }
  else{
    param_set$default[i] <- "NA"
  }
}

params <- data.frame(class = unname(learner$param_set$class), lower = unname(learner$param_set$lower), unname(learner$param_set$upper), levels = param_set$levels, default = param_set$default)
params
```


In particular, we focus on the following parameters:
- `family` is a symbolic description of the loss function and corresponding risk functions to be optimized by blackboost [default: coxph].
- `mstop` is an integer giving the number of initial boosting iterations. If mstop = 0, the offset model is returned (in our case, a tree is returned) [default: 100].
- `nu` is a double (between 0 and 1) defining the step size or shrinkage parameter [default: 0.1].
- `stopintern` is a logical that defines if the boosting algorithm stops internally when the outof-bag risk in one iteration is larger than the out-of-bag risk in the iteration before. Can also be a positive number giving the risk difference that needs to be exceeded [default: FALSE].
- `maxdepth` is the maximum depth of the tree [default: Inf]. 
- `trace` is a logical triggering printout of status information during the fitting process; it is just for printing out information [default: FALSE].

In order to set the learner parameters it is necessary to use the following specification, changing "..." with the wanted value.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
learner$param_set$values <- list(family = ..., 
                                 mstop = ..., 
                                 nu = ..., 
                                 stopintern = ...,
                                 trace = ..., 
                                 maxdepth = ...)
```

If one is interested in setting other parameters we don't cover in this tutorial, we suggest to check these CRAN documentations:
- blackboost parameters: https://cran.r-project.org/web/packages/mboost/mboost.pdf under "blackboost" section;
- boosting parameters: https://cran.r-project.org/web/packages/mboost/mboost.pdf under "boost_control" section;
- tree parameters: https://cran.r-project.org/web/packages/partykit/partykit.pdf under "ctree_controls" section.
Please notice that these documentations do not refer directly to blackboost learner parameters, but they give an idea on what the parameter represents and on how to tune it correctly.

##Tuning Parameters
Before fitting out model to the training data, we compare different specifications of the hyperparameters that we believe can improve the predictive performance of our model. We define a *tuning instance* that we will later feed to the optimization algorithm of our choice. We  specify the dataset (or a *task* according to `mlr3proba` notation) on which tuning has to be performed, the *learner* that we are using and the *search space*, which includes lower and upper bounds for out hyperparameters. Next, we need to indicate the *measure* and the *resampling* strategy according to which the performance of our model needs to be evaluated. Finally, we impose a *Terminator* or a budget representing the criterium according to which the optimization algorithm will stop.

```{r, results='hide'}
search_space = ps(
  mstop = p_int(lower = 70, upper = 130),
  nu = p_dbl(lower = 0.01, upper = 0.1),
  maxdepth = p_int(lower = 2, upper = 8),
  stopintern = p_lgl())
CVstrat = rsmp("cv", folds = 10)
measure = msr("surv.cindex")
stagTerm <- trm("stagnation")
```
___

Regarding the `terminator` of our algorithm, `mlr3tuning` provides different options:
-Terminate after a given time (TerminatorClockTime)
-Terminate after a given amount of iterations (TerminatorEvals)
-Terminate after a specific performance is reached (TerminatorPerfReached)
-Terminate when tuning does not improve (TerminatorStagnation)
-A combination of the above in an ALL or ANY fashion (TerminatorCombo)
(I must reference it.. it is taken from https://mlr3book.mlr-org.com/tuning.html)

Here we present an example in which we let termination an open choice among Stagnation and Evals:

```{r, results='hide'}
combo <- trm("combo",
              list(trm("evals", n_evals = 50),
                   trm("stagnation")),
              any = TRUE
              )
```
___
Let us take a look at what our tuning instance looks like:
```{r}
instance = TuningInstanceSingleCrit$new(
  task = train_task,
  measure = measure,
  learner = learner,
  resampling = CVstrat,
  search_space = search_space,
  terminator = combo
)
```
___
```{r}
instance
```
___
Finally, we can specify the optimization algorithm to be implemented on the `instance` via the `tuner` class:
```{r, results='hide'}
tuner = tnr("random_search")
tuner$optimize(instance)
```
___
We can now store the results in the `instance` object in order to train our model with the tuned values of the parameters of interest:
```{r, results='hide'}
instance$result_learner_param_vals
learner$param_set$values = instance$result_learner_param_vals
```


## Benchmarking
Once the new learner is trained with the optimized parameters, we are of course interested in comparing its performance with both the base blackboosting and the benchmark Cox regression.
The mlr3 framework provides a convenient way of doing so through the `mlr3benchmark` package, which compares the performance of multiple learners on a common task with the same training and test data.

This package exploits two main functions:
  - `benchmark_grid()` allows to set up a customized benchmarking design, 
      composed of a combination of `Task`, `Learner`, and `Resampling` objects.
  - `benchmark()` performs the benchmarking according to the specified design and returns a <BenchmarkResult>.

___
First, we define the triplet of objects that will make up the design.
```{r}
# learners to evaluate
learners = c(learner.cox,learner.bb,learner.bb.opt)
print(learners)

# task with whole dataset
gbcs_task = TaskSurv$new(id = "gbcs", 
                         backend = gbcs2, 
                         time = "survtime", event = "censdead")

# resampling method
cv3 = rsmps("cv", folds = 3)
```

___
Then, we build the benchmark design and pass it to `benchmark()` to implement the benchmarking.
```{r}
set.seed(1)
design <- benchmark_grid(tasks = gbcs_task,
                         learners = learners,
                         resamplings = cv3)
benchmark <- benchmark(design)
```

___
We can now specify the performance measures according to which the comparison should be carried out. 
Here we define two measures, Harrell's C-index on training set and on test set. 
```{r}
measures = list(
  msr("surv.cindex", predict_sets = "train", id = "cindex_train"),
  msr("surv.cindex", id = "cindex_test")
)
```
Notice that we can state the set on which the measure should be computed only because we passed the `predict_sets = c("train", "test")` argument when building our learners with `lrn()`.

___
Finally, we can aggregate the cross-validated C-indices, and print a table displaying the test and train results for each learner.
```{r}
performance_tab <- benchmark$aggregate(measures)
print(performance_tab)
```
___
Thanks to the `mlr3viz` package it is also possible to graphically display the benchmarking results.
```{r}
library(mlr3viz)
autoplot(benchmark)
```







